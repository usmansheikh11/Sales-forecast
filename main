# Store Sales Time Series Forecasting
# Kaggle Project: Sales Forecasting for next 15 days

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import seaborn as sns
from datetime import datetime, timedelta
import warnings
from tqdm.notebook import tqdm  # For progress bars
warnings.filterwarnings('ignore')

# For time series models
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from prophet import Prophet

# For metrics
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Reading the datasets
print("Loading datasets...")
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
stores = pd.read_csv('stores.csv')
oil = pd.read_csv('oil.csv')
holidays_events = pd.read_csv('holidays_events.csv')
sample_submission = pd.read_csv('sample_submission.csv')

# Display basic information
print("\nTrain data shape:", train.shape)
print("Test data shape:", test.shape)
print("Sample submission shape:", sample_submission.shape)

# Convert date columns to datetime format
train['date'] = pd.to_datetime(train['date'])
test['date'] = pd.to_datetime(test['date'])
oil['date'] = pd.to_datetime(oil['date'])
holidays_events['date'] = pd.to_datetime(holidays_events['date'])

# Check for missing values
print("\nMissing values in train:", train.isnull().sum().sum())
print("Missing values in test:", test.isnull().sum().sum())
print("Missing values in oil:", oil.isnull().sum())

# Handle missing oil prices more robustly
print("Handling missing oil prices...")
# First check if there are any NaN values
if oil['dcoilwtico'].isnull().any():
    # Forward fill
    oil['dcoilwtico'] = oil['dcoilwtico'].fillna(method='ffill')
    # Backward fill any remaining NaNs (in case NaNs at the beginning)
    oil['dcoilwtico'] = oil['dcoilwtico'].fillna(method='bfill')
    # If still have NaNs, use mean (unlikely but just in case)
    if oil['dcoilwtico'].isnull().any():
        mean_oil = oil['dcoilwtico'].mean()
        oil['dcoilwtico'] = oil['dcoilwtico'].fillna(mean_oil)
        print(f"Filled remaining NaNs with mean oil price: {mean_oil}")

    print("After filling, missing values in oil:", oil.isnull().sum())

# Data Exploration
print("\nExploring the data...")

# Find the date range in training data
min_date = train['date'].min()
max_date = train['date'].max()
print(f"Training data date range: {min_date} to {max_date}")

# Confirm the test data follows immediately after
min_test_date = test['date'].min()
max_test_date = test['date'].max()
print(f"Test data date range: {min_test_date} to {max_test_date}")
print(f"Number of days in test data: {(max_test_date - min_test_date).days + 1}")

# Analyze the stores
print("\nStore types distribution:")
print(stores['type'].value_counts())

# Analyze product families
print("\nProduct families:")
print(train['family'].nunique())
print(train['family'].value_counts().head())

# Data visualization section
print("\nCreating visualizations...")

# Plot total sales over time (aggregated for all stores and products)
sales_over_time = train.groupby('date')['sales'].sum().reset_index()

fig = px.line(sales_over_time, x='date', y='sales', title='Total Sales Over Time')
fig.update_layout(xaxis_title='Date', yaxis_title='Total Sales')
fig.show()

# Plot sales by store type
# Merge train with stores to get store types
train_with_stores = train.merge(stores, on='store_nbr', how='left')
sales_by_store_type = train_with_stores.groupby(['date', 'type'])['sales'].sum().reset_index()

fig = px.line(sales_by_store_type, x='date', y='sales', color='type',
              title='Sales by Store Type')
fig.update_layout(xaxis_title='Date', yaxis_title='Total Sales')
fig.show()

# Plot oil price over time and its correlation with sales
fig = make_subplots(specs=[[{"secondary_y": True}]])

fig.add_trace(
    go.Scatter(x=sales_over_time['date'], y=sales_over_time['sales'], name="Sales"),
    secondary_y=False,
)

oil_in_range = oil[(oil['date'] >= min_date) & (oil['date'] <= max_date)]
fig.add_trace(
    go.Scatter(x=oil_in_range['date'], y=oil_in_range['dcoilwtico'], name="Oil Price"),
    secondary_y=True,
)

fig.update_layout(
    title_text="Sales and Oil Price Over Time",
    xaxis_title="Date",
)
fig.update_yaxes(title_text="Sales", secondary_y=False)
fig.update_yaxes(title_text="Oil Price (USD)", secondary_y=True)
fig.show()

# Analyze holidays impact
# Filter transferred holidays
holidays_actual = holidays_events[holidays_events['type'] != 'Transfer']
holidays_dict = {date: type for date, type in zip(holidays_actual['date'], holidays_actual['type'])}

# Mark days in the sales dataset that are holidays
sales_over_time['holiday'] = sales_over_time['date'].apply(lambda x: 'Holiday' if x in holidays_dict else 'Regular Day')

# Plot sales comparing holidays vs regular days
fig = px.box(sales_over_time, x='holiday', y='sales', title='Sales: Holidays vs Regular Days')
fig.update_layout(xaxis_title='Day Type', yaxis_title='Total Sales')
fig.show()

# Data Preprocessing for modeling
print("\nPreprocessing data for modeling...")

# We'll focus on total daily sales for the forecasting
daily_sales = train.groupby('date')['sales'].sum().reset_index()
daily_sales = daily_sales.set_index('date')

# Add features from other datasets
# Add oil prices
daily_sales = daily_sales.join(oil.set_index('date')['dcoilwtico'], how='left')
# Forward fill then backward fill to ensure no NaN values remain
daily_sales['dcoilwtico'] = daily_sales['dcoilwtico'].fillna(method='ffill').fillna(method='bfill')
# If still any NaNs (unlikely), replace with mean
if daily_sales['dcoilwtico'].isnull().any():
    mean_oil_price = daily_sales['dcoilwtico'].mean()
    daily_sales['dcoilwtico'] = daily_sales['dcoilwtico'].fillna(mean_oil_price)

# Add holiday indicator
daily_sales['is_holiday'] = daily_sales.index.map(lambda x: 1 if x in holidays_dict else 0)

# Add day of week, month, year features
daily_sales['dayofweek'] = daily_sales.index.dayofweek
daily_sales['month'] = daily_sales.index.month
daily_sales['year'] = daily_sales.index.year

# Display the prepared dataset
print("\nPrepared daily sales dataset:")
print(daily_sales.head())

# Time Series Analysis
print("\nPerforming time series analysis...")

# Check stationarity
result = adfuller(daily_sales['sales'].dropna())
print('ADF Statistic:', result[0])
print('p-value:', result[1])
print('Critical Values:', result[4])
if result[1] <= 0.05:
    print("The series is stationary")
else:
    print("The series is not stationary")

# Apply differencing if needed
if result[1] > 0.05:
    daily_sales['sales_diff'] = daily_sales['sales'].diff().fillna(0)
    result = adfuller(daily_sales['sales_diff'].dropna())
    print('\nAfter differencing:')
    print('ADF Statistic:', result[0])
    print('p-value:', result[1])
    if result[1] <= 0.05:
        print("The differenced series is stationary")
    else:
        print("The differenced series is still not stationary")

# Decompose the time series to see trend, seasonality, and residuals
decomposition = sm.tsa.seasonal_decompose(daily_sales['sales'], model='additive', period=7)
fig = make_subplots(rows=4, cols=1, shared_xaxes=True)
fig.add_trace(go.Scatter(x=daily_sales.index, y=daily_sales['sales'], name='Observed'), row=1, col=1)
fig.add_trace(go.Scatter(x=daily_sales.index, y=decomposition.trend, name='Trend'), row=2, col=1)
fig.add_trace(go.Scatter(x=daily_sales.index, y=decomposition.seasonal, name='Seasonal'), row=3, col=1)
fig.add_trace(go.Scatter(x=daily_sales.index, y=decomposition.resid, name='Residual'), row=4, col=1)
fig.update_layout(height=900, title_text="Time Series Decomposition")
fig.show()

# Generate dates for forecasting (15 days after the last date in training)
last_date = daily_sales.index.max()
forecast_dates = pd.date_range(start=last_date + timedelta(days=1), periods=15)
print("\nForecast dates:")
print(forecast_dates)

# Prepare test data features
forecast_df = pd.DataFrame(index=forecast_dates)
forecast_df['dayofweek'] = forecast_df.index.dayofweek
forecast_df['month'] = forecast_df.index.month
forecast_df['year'] = forecast_df.index.year

# Add oil prices to forecast dataframe
forecast_oil = oil[oil['date'].isin(forecast_dates)].set_index('date')['dcoilwtico']
forecast_df = forecast_df.join(forecast_oil, how='left')
forecast_df['dcoilwtico'] = forecast_df['dcoilwtico'].fillna(method='ffill')

# Add holiday indicator to forecast dataframe
forecast_df['is_holiday'] = forecast_df.index.map(lambda x: 1 if x in holidays_dict else 0)

print("\nPrepared forecast dataframe:")
print(forecast_df.head())

# Model 1: ARIMA
print("\nTraining ARIMA model...")
# Identify p, d, q parameters
# For simplicity, we'll use p=1, d=1, q=1 as a starting point
p, d, q = 1, 1, 1
arima_model = ARIMA(daily_sales['sales'], order=(p, d, q))
arima_results = arima_model.fit()
print(arima_results.summary())

# Forecast with ARIMA
arima_forecast = arima_results.forecast(steps=15)
print("\nARIMA forecast:")
print(arima_forecast)

# Model 2: SARIMA (Seasonal ARIMA)
print("\nTraining SARIMA model...")
# Identify p, d, q, P, D, Q, s parameters
# For simplicity, we'll use p=1, d=1, q=1, P=1, D=1, Q=1, s=7 (weekly seasonality)
p, d, q = 1, 1, 1
P, D, Q, s = 1, 1, 1, 7

# Check for NaN or infinite values before modeling
print("NaN values in dcoilwtico:", daily_sales['dcoilwtico'].isnull().sum())
print("NaN values in is_holiday:", daily_sales['is_holiday'].isnull().sum())

# Create a copy of the exogenous variables to ensure they're clean
exog_vars = daily_sales[['dcoilwtico', 'is_holiday']].copy()
# Double-check and handle any remaining NaN or infinite values
exog_vars = exog_vars.replace([np.inf, -np.inf], np.nan)
if exog_vars.isnull().any().any():
    print("Warning: Found NaN values in exogenous variables. Filling them.")
    exog_vars = exog_vars.fillna(exog_vars.mean())

# Similarly, clean forecast exog data
forecast_exog = forecast_df[['dcoilwtico', 'is_holiday']].copy()
forecast_exog = forecast_exog.replace([np.inf, -np.inf], np.nan)
if forecast_exog.isnull().any().any():
    print("Warning: Found NaN values in forecast exogenous variables. Filling them.")
    forecast_exog = forecast_exog.fillna(forecast_exog.mean())

try:
    sarima_model = SARIMAX(daily_sales['sales'],
                           order=(p, d, q),
                           seasonal_order=(P, D, Q, s),
                           exog=exog_vars)
    sarima_results = sarima_model.fit(disp=False)
    print(sarima_results.summary())

    # Forecast with SARIMA
    sarima_forecast = sarima_results.forecast(steps=15, exog=forecast_exog)
except Exception as e:
    print(f"Error during SARIMA modeling: {e}")
    print("Falling back to SARIMA without exogenous variables")

    # Fallback: SARIMA without exogenous variables
    sarima_model = SARIMAX(daily_sales['sales'],
                           order=(p, d, q),
                           seasonal_order=(P, D, Q, s))
    sarima_results = sarima_model.fit(disp=False)
    print(sarima_results.summary())

    # Forecast with simpler SARIMA
    sarima_forecast = sarima_results.forecast(steps=15)
print("\nSARIMA forecast:")
print(sarima_forecast)

# Model 3: Prophet
print("\nTraining Prophet model...")
# Prepare data for Prophet
prophet_data = daily_sales.reset_index()
prophet_data = prophet_data.rename(columns={'date': 'ds', 'sales': 'y'})

# Add additional regressors - ensure they're completely clean of NaNs
prophet_data['oil_price'] = prophet_data['dcoilwtico']
prophet_data['holiday'] = prophet_data['is_holiday']

# Check for NaN values in Prophet data
print("NaN values in Prophet data before cleaning:")
print(prophet_data[['ds', 'y', 'oil_price', 'holiday']].isnull().sum())

# Clean all data for Prophet - Prophet is especially sensitive to missing values
prophet_data = prophet_data.replace([np.inf, -np.inf], np.nan)

# For y (sales), use interpolation
prophet_data['y'] = prophet_data['y'].interpolate(method='linear').fillna(method='bfill').fillna(method='ffill')

# For regressors, handle specifically
for col in ['oil_price', 'holiday']:
    if prophet_data[col].isnull().any():
        print(f"Filling NaN values in {col}")
        # First try interpolation
        prophet_data[col] = prophet_data[col].interpolate(method='linear')
        # Then try forward fill
        prophet_data[col] = prophet_data[col].fillna(method='ffill')
        # Then try backward fill
        prophet_data[col] = prophet_data[col].fillna(method='bfill')
        # If still have NaNs, use mean
        if prophet_data[col].isnull().any():
            mean_val = prophet_data[col].dropna().mean()
            print(f"Using mean value {mean_val} for remaining NaNs in {col}")
            prophet_data[col] = prophet_data[col].fillna(mean_val)

# Double check after cleaning
print("NaN values in Prophet data after cleaning:")
print(prophet_data[['ds', 'y', 'oil_price', 'holiday']].isnull().sum())

# Make sure all data is numeric and not object type
prophet_data['oil_price'] = pd.to_numeric(prophet_data['oil_price'], errors='coerce')
prophet_data['holiday'] = pd.to_numeric(prophet_data['holiday'], errors='coerce')

# Final fill of any values that might have been coerced to NaN
prophet_data['oil_price'] = prophet_data['oil_price'].fillna(prophet_data['oil_price'].mean())
prophet_data['holiday'] = prophet_data['holiday'].fillna(0)

try:
    # First try with both regressors
    print("Fitting Prophet model with regressors...")
    prophet_model = Prophet(yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=False)
    prophet_model.add_regressor('oil_price')
    prophet_model.add_regressor('holiday')
    prophet_model.fit(prophet_data[['ds', 'y', 'oil_price', 'holiday']])

    print("Creating future dataframe...")
    # Prepare future dataframe for Prophet
    future = prophet_model.make_future_dataframe(periods=15)

    # Clean forecast data
    forecast_df_prophet = forecast_df.copy()
    forecast_df_prophet = forecast_df_prophet.replace([np.inf, -np.inf], np.nan)

    # Ensure forecast oil data has no NaNs
    if forecast_df_prophet['dcoilwtico'].isnull().any():
        last_valid_oil = prophet_data['oil_price'].iloc[-1]
        forecast_df_prophet['dcoilwtico'] = forecast_df_prophet['dcoilwtico'].fillna(last_valid_oil)

    print("Adding regressors to future dataframe...")
    # Add oil_price to future
    oil_past = prophet_data['oil_price'].values
    oil_future = forecast_df_prophet['dcoilwtico'].values

    # Fix length issues
    while len(oil_future) < 15:
        oil_future = np.append(oil_future, oil_future[-1] if len(oil_future) > 0 else oil_past[-1])
    oil_future = oil_future[:15]

    # Create full oil_price series
    future['oil_price'] = np.nan  # Initialize with NaN
    future.loc[:len(oil_past)-1, 'oil_price'] = oil_past
    future.loc[len(oil_past):, 'oil_price'] = oil_future

    # Add holiday to future
    holiday_past = prophet_data['holiday'].values
    holiday_future = forecast_df_prophet['is_holiday'].values

    # Fix length issues
    while len(holiday_future) < 15:
        holiday_future = np.append(holiday_future, 0)
    holiday_future = holiday_future[:15]

    # Create full holiday series
    future['holiday'] = np.nan  # Initialize with NaN
    future.loc[:len(holiday_past)-1, 'holiday'] = holiday_past
    future.loc[len(holiday_past):, 'holiday'] = holiday_future

    # Final check for NaNs in future
    if future[['oil_price', 'holiday']].isnull().any().any():
        print("Warning: NaNs still present in future dataframe. Fixing...")
        future['oil_price'] = future['oil_price'].fillna(method='ffill').fillna(method='bfill')
        future['holiday'] = future['holiday'].fillna(0)

    print("Making Prophet forecast...")
    # Make forecast
    prophet_forecast = prophet_model.predict(future)

except Exception as e:
    print(f"Error with full Prophet model: {e}")
    print("Falling back to simpler Prophet model without regressors")

    # Fallback to simpler model
    prophet_model = Prophet(yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=False)
    prophet_model.fit(prophet_data[['ds', 'y']])

    # Prepare future dataframe for Prophet without regressors
    future = prophet_model.make_future_dataframe(periods=15)
    prophet_forecast = prophet_model.predict(future)

# Forecast with Prophet
prophet_forecast = prophet_model.predict(future)
print("\nProphet forecast:")
print(prophet_forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(15))

# Visualize Prophet components
fig = prophet_model.plot_components(prophet_forecast)
plt.show()

# Compare forecasts visually
fig = go.Figure()

# Add historical data
fig.add_trace(go.Scatter(
    x=daily_sales.index,
    y=daily_sales['sales'],
    mode='lines',
    name='Historical Sales'
))

# Add ARIMA forecast
fig.add_trace(go.Scatter(
    x=forecast_dates,
    y=arima_forecast,
    mode='lines',
    name='ARIMA Forecast',
    line=dict(dash='dash')
))

# Add SARIMA forecast
fig.add_trace(go.Scatter(
    x=forecast_dates,
    y=sarima_forecast,
    mode='lines',
    name='SARIMA Forecast',
    line=dict(dash='dot')
))

# Add Prophet forecast
fig.add_trace(go.Scatter(
    x=prophet_forecast.tail(15)['ds'],
    y=prophet_forecast.tail(15)['yhat'],
    mode='lines',
    name='Prophet Forecast',
    line=dict(dash='longdash')
))

# Add confidence intervals for Prophet
fig.add_trace(go.Scatter(
    x=prophet_forecast.tail(15)['ds'].tolist() + prophet_forecast.tail(15)['ds'].tolist()[::-1],
    y=prophet_forecast.tail(15)['yhat_upper'].tolist() + prophet_forecast.tail(15)['yhat_lower'].tolist()[::-1],
    fill='toself',
    fillcolor='rgba(0,176,246,0.2)',
    line=dict(color='rgba(255,255,255,0)'),
    name='Prophet Confidence Interval'
))

fig.update_layout(
    title='Sales Forecasts Comparison: ARIMA, SARIMA, and Prophet',
    xaxis_title='Date',
    yaxis_title='Sales',
    legend_title='Models',
    hovermode='x unified'
)
fig.show()

# Now we need to prepare forecasts for individual store-product combinations
# and create a submission file

print("\nPreparing submission file...")

# Create an ensemble forecast by averaging the three models
# (or using available models if some failed)
print("\nCreating ensemble forecast...")

ensemble_forecasts = pd.DataFrame(index=forecast_dates)

# Add ARIMA forecasts
try:
    # Check for NaN or infinite values
    if np.isnan(arima_forecast).any() or np.isinf(arima_forecast).any():
        print("Warning: ARIMA forecast contains NaN or infinite values. Fixing...")
        # Replace infinites with NaN then interpolate
        arima_clean = pd.Series(arima_forecast).replace([np.inf, -np.inf], np.nan)
        arima_clean = arima_clean.interpolate().fillna(method='ffill').fillna(method='bfill')
        # If still have NaNs, use mean
        if arima_clean.isna().any():
            arima_clean = arima_clean.fillna(arima_clean.mean())
        ensemble_forecasts['arima'] = arima_clean.values
    else:
        ensemble_forecasts['arima'] = arima_forecast.values
except Exception as e:
    print(f"Could not add ARIMA to ensemble: {e}")

# Add SARIMA forecasts if available
try:
    # Check for NaN or infinite values
    if np.isnan(sarima_forecast).any() or np.isinf(sarima_forecast).any():
        print("Warning: SARIMA forecast contains NaN or infinite values. Fixing...")
        # Replace infinites with NaN then interpolate
        sarima_clean = pd.Series(sarima_forecast).replace([np.inf, -np.inf], np.nan)
        sarima_clean = sarima_clean.interpolate().fillna(method='ffill').fillna(method='bfill')
        # If still have NaNs, use mean
        if sarima_clean.isna().any():
            sarima_clean = sarima_clean.fillna(sarima_clean.mean())
        ensemble_forecasts['sarima'] = sarima_clean.values
    else:
        ensemble_forecasts['sarima'] = sarima_forecast.values
except Exception as e:
    print(f"Could not add SARIMA to ensemble: {e}")

# Add Prophet forecasts if available
try:
    prophet_values = prophet_forecast.tail(15)['yhat'].values

    # Check for NaN or infinite values
    if np.isnan(prophet_values).any() or np.isinf(prophet_values).any():
        print("Warning: Prophet forecast contains NaN or infinite values. Fixing...")
        # Replace infinites with NaN then interpolate
        prophet_clean = pd.Series(prophet_values).replace([np.inf, -np.inf], np.nan)
        prophet_clean = prophet_clean.interpolate().fillna(method='ffill').fillna(method='bfill')
        # If still have NaNs, use mean
        if prophet_clean.isna().any():
            prophet_clean = prophet_clean.fillna(prophet_clean.mean())
        ensemble_forecasts['prophet'] = prophet_clean.values
    else:
        ensemble_forecasts['prophet'] = prophet_values
except Exception as e:
    print(f"Could not add Prophet to ensemble: {e}")

# Create ensemble by averaging available forecasts
available_models = ensemble_forecasts.columns
if len(available_models) > 0:
    ensemble_forecasts['ensemble'] = ensemble_forecasts[available_models].mean(axis=1)
    print(f"Ensemble created using models: {', '.join(available_models)}")
else:
    print("No models available for ensemble. Creating simple forecast.")
    # Create a simple forecast based on average daily sales
    avg_sales = daily_sales['sales'].mean()
    ensemble_forecasts['ensemble'] = [avg_sales] * len(forecast_dates)

# Plot the ensemble forecast
fig = go.Figure()

# Add historical data
fig.add_trace(go.Scatter(
    x=daily_sales.index,
    y=daily_sales['sales'],
    mode='lines',
    name='Historical Sales'
))

# Add ensemble forecast
fig.add_trace(go.Scatter(
    x=forecast_dates,
    y=ensemble_forecasts['ensemble'],
    mode='lines',
    name='Ensemble Forecast',
    line=dict(width=3, color='green')
))

fig.update_layout(
    title='Ensemble Sales Forecast',
    xaxis_title='Date',
    yaxis_title='Sales',
    legend_title='Data',
    hovermode='x unified'
)
fig.show()

# Calculate the average proportion of sales for each store-product combination
print("\nCalculating proportions for store-product combinations...")
store_family_props = train.groupby(['store_nbr', 'family'])['sales'].sum()
total_sales = store_family_props.sum()
store_family_props = store_family_props / total_sales

# Match the test dataset with our forecasts
test['date'] = pd.to_datetime(test['date'])
test = test.sort_values(['date', 'store_nbr', 'family'])

# Get unique dates in test set
test_dates = test['date'].unique()

# For each date, get the forecasted total sales and distribute to store-family combinations
submission = pd.DataFrame()
submission['id'] = test['id']

# Get Ensemble forecasts for each date
ensemble_forecast_dict = dict(zip(forecast_dates, ensemble_forecasts['ensemble']))

# Calculate sales for each store-family combination
print("Generating predictions for each store-product combination...")
sales_values = []
for _, row in tqdm(test.iterrows(), total=len(test), desc="Processing test rows"):
    date = row['date']
    store_nbr = row['store_nbr']
    family = row['family']

    # Get total forecasted sales for this date
    total_forecast = ensemble_forecast_dict.get(date, 0)

    # Get proportion for this store-family
    try:
        proportion = store_family_props.loc[(store_nbr, family)]
    except KeyError:
        proportion = 0

    # Calculate sales for this store-family
    sales = total_forecast * proportion

    # Ensure sales are not negative
    sales = max(0, sales)

    sales_values.append(sales)

submission['sales'] = sales_values

# Save submission file
submission.to_csv('sales_forecast_submission.csv', index=False)
print("\nSubmission file generated: sales_forecast_submission.csv")

print("\nDone!")
